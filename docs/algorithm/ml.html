
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Machine Learning Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../math/" />
    
    
    <link rel="prev" href="cryptography.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../python/">
            
                <a href="../python/">
            
                    
                    Python
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../python/standard-library.html">
            
                <a href="../python/standard-library.html">
            
                    
                    Python Stardard Library
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../python/tools-and-packages.html">
            
                <a href="../python/tools-and-packages.html">
            
                    
                    Tools & More Packages
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../python/django.html">
            
                <a href="../python/django.html">
            
                    
                    Django
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../java/">
            
                <a href="../java/">
            
                    
                    Java
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../java/javafx.html">
            
                <a href="../java/javafx.html">
            
                    
                    JavaFX
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../java/android.md">
            
                <span>
            
                    
                    Android Studio
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../javascript/">
            
                <a href="../javascript/">
            
                    
                    JavaScript
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../javascript/jquery.html">
            
                <a href="../javascript/jquery.html">
            
                    
                    jQuery
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../javascript/react.html">
            
                <a href="../javascript/react.html">
            
                    
                    React
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../javascript/react-native.html">
            
                <a href="../javascript/react-native.html">
            
                    
                    React Native
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../javascript/node-js.html">
            
                <a href="../javascript/node-js.html">
            
                    
                    Node.js
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../javascript/wechat-mini-program.html">
            
                <a href="../javascript/wechat-mini-program.html">
            
                    
                    Wechat Mini Program
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../c/">
            
                <a href="../c/">
            
                    
                    C
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../cpp/">
            
                <a href="../cpp/">
            
                    
                    C++
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../c-sharp/">
            
                <a href="../c-sharp/">
            
                    
                    C#
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../c-sharp/dot-net.html">
            
                <a href="../c-sharp/dot-net.html">
            
                    
                    .NET
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../c-sharp/asp-dot-net.html">
            
                <a href="../c-sharp/asp-dot-net.html">
            
                    
                    ASP.NET
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../swift/">
            
                <a href="../swift/">
            
                    
                    Swift
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../swift/xcode.html">
            
                <a href="../swift/xcode.html">
            
                    
                    Xcode
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../markup/">
            
                <a href="../markup/">
            
                    
                    Markup Language
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../markup/html.html">
            
                <a href="../markup/html.html">
            
                    
                    HTML
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../markup/xml.html">
            
                <a href="../markup/xml.html">
            
                    
                    XML
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../markup/gfm.html">
            
                <a href="../markup/gfm.html">
            
                    
                    GFM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.4" data-path="../markup/latex.html">
            
                <a href="../markup/latex.html">
            
                    
                    LaTeX
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../css/">
            
                <a href="../css/">
            
                    
                    CSS
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="../css/bootstrap.html">
            
                <a href="../css/bootstrap.html">
            
                    
                    Bootstrap
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../typescript/">
            
                <a href="../typescript/">
            
                    
                    TypeScript
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.11.1" data-path="../typescript/angular.html">
            
                <a href="../typescript/angular.html">
            
                    
                    Angular
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="../php/">
            
                <a href="../php/">
            
                    
                    PHP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.12.1" data-path="../php/wordpress.html">
            
                <a href="../php/wordpress.html">
            
                    
                    WordPress
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.13" data-path="../sql/">
            
                <a href="../sql/">
            
                    
                    SQL
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="../os/">
            
                <a href="../os/">
            
                    
                    Operating Systems
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.14.1" data-path="../os/linux.html">
            
                <a href="../os/linux.html">
            
                    
                    Linux
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.2" data-path="../os/bash-scripting.html">
            
                <a href="../os/bash-scripting.html">
            
                    
                    Bash Scripting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.3" data-path="../os/regex.html">
            
                <a href="../os/regex.html">
            
                    
                    Regular Expression
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.4" data-path="../os/macos.html">
            
                <a href="../os/macos.html">
            
                    
                    MacOS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.5" data-path="../os/solution-stack.html">
            
                <a href="../os/solution-stack.html">
            
                    
                    Solution Stack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.6" data-path="../os/hadoop.html">
            
                <a href="../os/hadoop.html">
            
                    
                    Hadoop
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.7" data-path="../os/git.html">
            
                <a href="../os/git.html">
            
                    
                    Git
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14.8" data-path="../os/redis.html">
            
                <a href="../os/redis.html">
            
                    
                    Redis
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="../networking/">
            
                <a href="../networking/">
            
                    
                    Networking
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="../networking/security.html">
            
                <a href="../networking/security.html">
            
                    
                    Security
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="../networking/cisco.html">
            
                <a href="../networking/cisco.html">
            
                    
                    Cisco
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="../virtualization/">
            
                <a href="../virtualization/">
            
                    
                    Virtualization
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.16.1" data-path="../virtualization/virtualbox.html">
            
                <a href="../virtualization/virtualbox.html">
            
                    
                    VirtualBox
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.2" data-path="../virtualization/docker.html">
            
                <a href="../virtualization/docker.html">
            
                    
                    Docker
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.3" data-path="../virtualization/kubernetes.html">
            
                <a href="../virtualization/kubernetes.html">
            
                    
                    Kubernetes
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.17" data-path="../cloud-services/">
            
                <a href="../cloud-services/">
            
                    
                    Cloud Services
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.17.1" data-path="../cloud-services/aws.html">
            
                <a href="../cloud-services/aws.html">
            
                    
                    AWS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.2" data-path="../cloud-services/gcp.html">
            
                <a href="../cloud-services/gcp.html">
            
                    
                    GCP
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.18" data-path="../embedded/">
            
                <a href="../embedded/">
            
                    
                    Embedded Systems
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.18.1" data-path="../embedded/arduino.html">
            
                <a href="../embedded/arduino.html">
            
                    
                    Arduino
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18.2" data-path="../embedded/raspberry-pi.html">
            
                <a href="../embedded/raspberry-pi.html">
            
                    
                    Raspberry Pi
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18.3" data-path="../embedded/st.html">
            
                <a href="../embedded/st.html">
            
                    
                    STMicroelectronics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.18.4" data-path="../embedded/electricity-magnetism.html">
            
                <a href="../embedded/electricity-magnetism.html">
            
                    
                    Electricity & Magnetism
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.19" data-path="../gis/">
            
                <a href="../gis/">
            
                    
                    GIS
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.19.1" data-path="../gis/arcgis.html">
            
                <a href="../gis/arcgis.html">
            
                    
                    ArcGIS
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.20" data-path="./">
            
                <a href="./">
            
                    
                    Algorithm
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.20.1" data-path="blockchain.html">
            
                <a href="blockchain.html">
            
                    
                    Blockchain
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.20.2" data-path="cryptography.html">
            
                <a href="cryptography.html">
            
                    
                    Cryptography
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.20.3" data-path="ml.html">
            
                <a href="ml.html">
            
                    
                    Machine Learning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.21" data-path="../math/">
            
                <a href="../math/">
            
                    
                    Mathematics
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.21.1" data-path="../math/calculus.html">
            
                <a href="../math/calculus.html">
            
                    
                    Calculus
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.2" data-path="../math/linear-algebra.html">
            
                <a href="../math/linear-algebra.html">
            
                    
                    Linear Algebra
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.3" data-path="../math/discrete.html">
            
                <a href="../math/discrete.html">
            
                    
                    Discrete Mathematics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.4" data-path="../math/probability-statistics.html">
            
                <a href="../math/probability-statistics.html">
            
                    
                    Probability & Statistics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.21.5" data-path="../math/maple.html">
            
                <a href="../math/maple.html">
            
                    
                    Maple Softwares
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.22" data-path="../graphic-design/">
            
                <a href="../graphic-design/">
            
                    
                    Graphic Design
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.23" data-path="../music/">
            
                <a href="../music/">
            
                    
                    Music Theory
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Machine Learning</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="machine-learning">Machine Learning</h1>
<h2 id="introduction">Introduction</h2>
<ul>
<li>Machines learning is one type of artificial intelligence.<ul>
<li>AI is a technique which enables machines to mimic human behaviour.</li>
</ul>
</li>
<li>Machine Learning has 4 categories:<ul>
<li><code>Supervised Learning</code> - Machine learns from data with labels, it can deals with the following tasks<ul>
<li>Classification<ul>
<li>Data are labeled manually with its class, expected to let the machine determine the class after learning from the data.</li>
</ul>
</li>
<li>Regression<ul>
<li>It is a method of predicting a numeric value called target.</li>
<li>A set of attributes with value or features are provided for learning, they are called predictors, along with target values as labels.<ul>
<li><code>attribute</code> is a type of information that will be learned by the machine.</li>
<li><code>feature</code> is a type of information with a value that will be learned by the machine.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>Unsupervised Learning</code> - data without label, it can deals with the following tasks.<ul>
<li>Clustering<ul>
<li>Grouping data</li>
</ul>
</li>
<li>Anomaly detection<ul>
<li>Find any data that is different from most of samples</li>
</ul>
</li>
<li>Novelty detection<ul>
<li>Find any data that has never been seen from all the sample data.</li>
</ul>
</li>
<li>Visualizetion<ul>
<li>Generates a 2D or 3D representation of input data points</li>
</ul>
</li>
<li>Dimensionality Reduction<ul>
<li>Sumarize data and make them have less attributes, as known as feature extraction.</li>
<li>It can be used to prepare data before putting into others machine learning algorithm.</li>
</ul>
</li>
<li>Association rule learning<ul>
<li>It is used to discover relations between attributes from different sample data.</li>
</ul>
</li>
</ul>
</li>
<li><code>Semisupervised learning</code> - little data with label, many other data without label.<ul>
<li>It uses a combination of both supervised learning algorithms and unsupervised learning algorithms .</li>
</ul>
</li>
<li><code>Reinforcement learning</code> - learning from experience or similar genetic algorithm.<ul>
<li>An agent will be given different situation. The agent will perform observation and perform an action. The agent will be rewarded and penalized accordingly. Policy will then be establish for it to get the most reward in different situation.</li>
</ul>
</li>
</ul>
</li>
<li>Any ML methods can be either one of the following type:<ul>
<li>online(stochastic) learning or batch learning<ul>
<li>whether or not the learning process is done at one time(batch learning) or consequently overtime based on a stream of incoming data(online learning)</li>
<li>batch learning is also called offline learning.</li>
<li>batch learning requires a lot of computing resources while online learning uses data as mini batch which can be cheap and fast.</li>
<li>online learning is vulnerable to bad data.</li>
</ul>
</li>
<li>instance-based learning or model-based learning<ul>
<li>instance-based will compare new data with sample data then make judgement.</li>
<li>model-based learning will generalize sample data, then processing new data based on the model.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="machine-learning-algorithms">Machine Learning Algorithms</h2>
<ul>
<li>Decision Trees and Random Forests</li>
<li>K-Means<ul>
<li>It is a unsupervised learning algorithm for clustering</li>
<li>It can identify discrete groupings within data</li>
</ul>
</li>
<li>DBSCAN</li>
<li>Hierarchical Cluster Analysis(HCA)<ul>
<li>Each Cluster group can have subgroups</li>
</ul>
</li>
<li>One-class SVM</li>
<li>Isolation Forest</li>
<li>Principal Component Analysis(PCA)</li>
<li>Kernel PCA</li>
<li>Locally Linear Embedding(LLE)</li>
<li>t-Distributed Stochastic Neighbor Embedding(t-SNE)</li>
<li>Apriori<ul>
<li>It is a type of association rules learning algorithm<ul>
<li>It can be used to find out the association of user behaviors and be used for task like movie recommandtion</li>
</ul>
</li>
<li>It calculates three values:<ul>
<li>Support: divide the number of users interested in the topic by the total number of users.</li>
<li>Confidence: divide the number of users interested in the related topic by the number of users interested in the original topic.</li>
<li>Lift, divide the confidence score by the support score.</li>
</ul>
</li>
<li>This algorithm compares all possible combination from the sample.<ul>
<li>Each combination is a potential rule.</li>
</ul>
</li>
<li>Only the samples have support and confidence scores higher than the minimum value set at the beginning of the calculation will be recorded.</li>
<li>The final result will rank the lift score from high to low.</li>
</ul>
</li>
<li>Eclat<ul>
<li>It is a type of association rules learning algorithm</li>
<li>It only has a support score which is calculated by dividing the rule set by the total number of samples</li>
<li>It also need to set a minimum support value and the support score will be ranked from high to low as the final result.</li>
</ul>
</li>
</ul>
<h3 id="linear-regression">Linear Regression</h3>
<ul>
<li>Linear regression is used to find the best fitting line(not nesscessirily straight) of a set of data points</li>
<li>suitable for solving regression problems that find a predicted value.</li>
<li>Ordinary least squares is used to calculate the best fit<ul>
<li>find the minimum value of the square of the difference of y and y hat.</li>
<li>y hat is the predicted value while y is the actual value.</li>
</ul>
</li>
<li>Simple linear regression - The model will be a linear function with one constant, one dependent variable and one independent variable <code>x</code> times a coefficient.</li>
<li>Multiple linear regression has one constant, one dependent variable and many combinations of independent variables times a coefficient.<ul>
<li>Each independent variable represents a different feature in a single row of data.</li>
<li>Dummy Variable - For a categorical featue each possible category will form a new field with value 0 or 1, when the current row of data is belonds this a certain category the value for this category will be 1 and others are 0.<ul>
<li>Dummy Variable Trap - The number of this categorical independent variable will always be one less than the total number of the possible options since all others are 0 implys this record belong to the last category already.</li>
</ul>
</li>
</ul>
</li>
<li>Polynomial(linear) regression - It is like Multiple linear regression but the power of independent variable increases with the number of them.</li>
<li>For linear regression models all of the following assumptions must be achieved.<ul>
<li>Linearity</li>
<li>Homoscedasticity</li>
<li>Multivariate normality</li>
<li>Independence of errors</li>
<li>Lack of multicollinearity</li>
</ul>
</li>
</ul>
<h3 id="decision-trees">Decision Trees</h3>
<ul>
<li>It can handle non-linear datasets.</li>
</ul>
<h5 id="regression-trees">Regression Trees</h5>
<ul>
<li>It is used for regression problems</li>
<li>It split the datapoints into groups based on the independent variables of a data point.</li>
<li>It uses an information entripy algorithm to optimaize the split.</li>
<li>Each one of the split is called a leaf.<ul>
<li>Each of the final split is called the terminal leaf.</li>
</ul>
</li>
<li>A decision tree is a binary tree structure, each node check if one of the independent variable is greater or smaller than a certain value.<ul>
<li>Nodes from the same depth are dealing with the same independent variable.</li>
</ul>
</li>
<li>The split add information to the datapoint and group them.</li>
<li>The average of all data points in one terminal leaf will be calculated as the prediced value for all new data point if it falls in the split.</li>
</ul>
<h5 id="classification-trees">Classification Trees</h5>
<ul>
<li>It is used for classification problems</li>
<li>similar to regression tree but each leaf represents one category</li>
<li>Doesn&apos;t have to go the terminal leaves for result, it can be stopped at any time and return the probobility for each classification result</li>
</ul>
<h3 id="random-forests">Random Forests</h3>
<ul>
<li>It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.<ul>
<li>A algorithm that utlizes a single machine learning algorithm multiple times or uses different algorithm at the same time.</li>
</ul>
</li>
<li>For complicated data, a single decision tree can easily get deep and overfitting.</li>
<li>It uses random subsets of the datapoints to form decision trees.<ul>
<li>The number of data points in each subset should be set.</li>
<li>The total numbers of trees should be set.</li>
</ul>
</li>
<li>It uses random combination of features of a data point.</li>
<li>It works for both classification tree and regression tree.<ul>
<li>For regresssion: the average of all split from each decision tree will be used to make prediction.</li>
<li>For classification: the most common categories result among all trees will be used as the final result.</li>
</ul>
</li>
</ul>
<h3 id="k-nearest-neighbors">k-Nearest Neighbors</h3>
<ul>
<li>A supervised learning algorithm</li>
<li>For Classification:<ul>
<li>Select a <code>k</code> value(small interger value). For any new data point find its <code>k</code> nearest neighbors, then the new data point belongs to the category that is the same as most of its neighbors.</li>
<li>Euclidean Distance(geometrical distance, square root of the sum of the square difference) is often used.</li>
</ul>
</li>
<li>For regression:<ul>
<li>Queries K-Nearest Neighbors and returns average value for the instance</li>
</ul>
</li>
<li>It does not scale well for large datasets</li>
</ul>
<h3 id="logistic-regression">Logistic Regression</h3>
<ul>
<li>It is a classification algorithm.</li>
<li>Fit a linear regression into a range of 0 and 1 as the probabilities using the logistic regression formula.<ul>
<li>Subsitute <code>y</code> in the linear regression formula into the sigmoid function will return the logistic regression formula.</li>
</ul>
</li>
<li>It can generate <code>P</code> hat as predicted probability of a given value.</li>
<li>It can return ethier 0 or 1 as <code>y</code> hat value if categorical results are preferred..<ul>
<li>return <code>y</code> hat as 1 if the probability is higher than a certain defined value and vise versa.</li>
</ul>
</li>
</ul>
<h3 id="support-vector-machinessvms">Support Vector Machines(SVMs)</h3>
<ul>
<li>It is a classificaton algorithm</li>
<li>It focuses on finding the boundary, then make decision only based on the boundary.</li>
<li>The boundary is found to separate all data point with maximum margin.<ul>
<li>Maximum margin means the greatest distance to points from different categories.</li>
</ul>
</li>
<li>The points that have the same cloest distance to the boundary is called support vectors.</li>
<li>The boundary is called maximum margin hyperplane or maximum margin classifier.</li>
<li>The lines which are parallel to the maximum margin hyperplane and have zero distance to the support vectors are called positive or negative hyperplane.</li>
</ul>
<h5 id="support-vector-regression">Support Vector Regression</h5>
<h3 id="neural-networks">Neural Networks</h3>
<ul>
<li>It was first introduced in 1980s, it was getting popular recently only becase the improved stroage and computing</li>
<li>Can be used for both regression and classification tasks.</li>
<li>Most Neural Networks are consist of a Feedforward Process and Backpropagation.<ul>
<li>FeedForward Process(Nerual Network) uses static mapping as the output only depend on the input and the weights</li>
<li>Going from output to input and finding the better weights for the model using stochastic gradient descent with the chain rule</li>
</ul>
</li>
<li>In neural science, each neuron has dedrites(receiver) and axon(transmitter), millions of millions of neuron work together. sigals are passed through synapses.</li>
<li>In neural networks, the processing unit is called neuron or node. The connection between neuron is also called synapse.</li>
<li>Overfitting and Underfitting - Underfitting(High Bias) implys a model that is too simply to make prediction, Overfitting(High Variance) implys a model that is learning too hard from the train data and too complicated to effectively and correctly make prediction.<ul>
<li>The goal is to aim to build a model that is a bit overfitting and slightly adjust it to prevent overfitting.</li>
<li>The most important indicator of overfitting is the train set accurancy is much better than the test set.</li>
</ul>
</li>
<li>Hyperparameters are parameters used to build and train models.</li>
</ul>
<h5 id="artificial-neural-networksanns">Artificial Neural Networks(ANNs)</h5>
<ul>
<li>The initial input values for the artificial neural network would be multiple features about one sample.</li>
<li>The final output values for the artificial neural network could be either a predicted value(one output), a boolean value(one output) or categorical values(multiple output nodes)</li>
<li>Each input value is associated with a weight, the neural network learns by adjusting the weight of each input value.</li>
<li>Each nerual will have the sum of all input value times the weight.</li>
<li>Activation function determine the output of each nerual based on the inputs&apos; sum value<ul>
<li>Threshold function - either yes or no(0 or 1)</li>
<li>Sigmoid function - gradually increase(0 to 1) close to but never reaches 0 or 1</li>
<li>Hyperbolic Tangent - gradually increase(-1 to 1) close to but never reaches -1 or 1</li>
<li>Rectifier function - either 0 or linearlly grows from 0 to 1.</li>
</ul>
</li>
<li>The input values are called the input layer the output values are called output layer.</li>
<li>Nodes in between are called hidden layers, each layers will process values and generate output for the next layer.<ul>
<li>Each neuron in the same layer focuses on different aspects of the input info.</li>
<li>Hidden layers can have as many as possible, the neural network can be called deep learning if there are lots of layers of neurons in between.</li>
<li>ANNs with fewer hidden layers can be called shallow learning.</li>
</ul>
</li>
<li>The final output will be compared with the labeled target value, and use back propagation to adjust all weights of synapse, in order to achieve a higher accuracy.<ul>
<li><code>y</code> is the actually value, <code>y</code> hat the is output value.</li>
<li>A cost function is used to calculate the difference between <code>y</code> and <code>y</code> hat. The most frenquely used cost function is mean squared error which divides the squares of the difference of <code>y</code> and <code>y</code> hat by 2.</li>
<li>During each back propagation all of the weight for each synapse in the entire neural network are adjusted at the same time.</li>
<li>All the weight are initially set to a random small number close to zero.</li>
<li>Learning rate can be set to control how much the weights can be adjusted at a time.</li>
</ul>
</li>
<li>A single layer feed forward nerual network(perceptron) is the base machine learning unit in ANNs.</li>
<li>There are several ways of adjust the weights:<ul>
<li>Try all possible combination of weight values and calculate its cost function value, this is not practically since it takes billions of years.(curse of dimensionality)</li>
<li>Gradient Descent - adjust the combination of weights by calculating the slope, hence each trials will move closer to the minimum value. The sum of cost functions of all rows of data in the dataset are calculated in one guess.<ul>
<li>It might find the local minimum if all the cost function values are not convex.</li>
</ul>
</li>
<li>Stochastic Gradient Descent - Similar to gradient descent, but make adjustment based on cost function of every single row of data.<ul>
<li>It will find the global minimum.</li>
<li>It is light weight and faster.</li>
</ul>
</li>
<li>Mini-batch Gradient Descent - runs one batch of rows at a time.</li>
</ul>
</li>
<li>One epoach is the process of finding minimun cost function value of a batch of dataset after many times of iteration of the forward propagation and backpropagation adjusting of the weights.<ul>
<li>One epoach will be repeated many times in order to improve accuracy.</li>
</ul>
</li>
</ul>
<h5 id="convolutional-neural-networkscnns">Convolutional Neural Networks(CNNs)</h5>
<ul>
<li>It is an ANN for image recoginion.</li>
<li>For a computer, a back and white image is an array of numbers from 0-255(8 bits data) where 0 represents black, 255 represents white and grey scale colors in between.</li>
<li>A colorful image is a 3-D array with 3 layers of 2-D values from 0-255. Each layer represents the intensity of red, green and blue. Each layer of data is called red, green and blue channel repectively.</li>
<li>CNNs will process an image in the following order:<ul>
<li>Convolution operation is to use a feature detector to generate a feature map.<ul>
<li>The feature detector is a small matrix(2-D array) that has a certain predefined value in it. Its has different sizes in various CNNs.</li>
<li>The feature detector(filter) will be moving across the image(all pixel will only be processed once, line by line) and count the number of matches(non-zero) found when it overlapse with the image.<ul>
<li>A stride the step the feature detector moves across the image.</li>
</ul>
</li>
<li>Feature detector has negtive value to dim the image pixel and positive value to highlight.</li>
</ul>
</li>
<li>A feature map(convolved feature) will be generated according to hte stride and feature detector size, the feature map will summarize the feature of the image.</li>
<li>Multiple feature detectors will be used on a single image and generate various feature maps that each emphasis on different features of the image.</li>
<li>Feature maps will be processed by rectifier function to generate rectifier linear unit layer breakup the linearality of feature maps and amplify the features for further process.<ul>
<li>Dark part of the feature map is represented by negtive value, bright part of the feature map is represented by positive value. Rectifier function remove the dark part of the feature map.</li>
</ul>
</li>
<li>Pooling will be used on the feature map to preserve the feature and remove unnesscery info to prevent overfitting(overfitting means too much info on one feature). There are serveral types of pooling.<ul>
<li>Max pooling(most used) - Using a small matrix to scan the feature map with certain stride line by line(all pixel will only be processed once). Only the max value will be recorded.</li>
<li>Average pooling - same process as max pooling, but record the average value.</li>
<li>Sum pooling - record the sum of the value.</li>
<li>Min pooling - record the minimum value only.</li>
</ul>
</li>
<li>Process from convolution to pooling can be repeat once again for some CNNs</li>
<li>The 2-D array result will be flatten to a 1-D array then it will be fed into an artificial neural network.<ul>
<li>Hidden layer in the ANN used by CNNs are called fully connected layers.</li>
<li>Back propagation in CNNs not only change weight, it also changes feature dectectors.</li>
</ul>
</li>
<li>The last fully connected layer will vote for the result for image recognition.</li>
<li>The softmax function is used to calculation possibilities of each result for image recogition so possibilities of all possible answers add up to 1.</li>
<li>The cost function in CNNs is called loss function, cross-entropy formula is frequenly used.</li>
<li>Famous CNNs Architectures<ul>
<li>CNNs Architectures are models with predefined structures</li>
<li><a href="https://homl.info/lenet5" target="_blank">LeNet-5</a><ul>
<li>Created by Yann LeCun in 1998 for MNIST dataset.</li>
</ul>
</li>
<li><a href="https://homl.info/80" target="_blank">AlexNet</a><ul>
<li>Won the 2012 ILSVRC challenge</li>
</ul>
</li>
<li><a href="https://homl.info/81" target="_blank">GoogLeNet</a><ul>
<li>Won the 2014 ILSVRC challenge</li>
</ul>
</li>
<li><a href="https://homl.info/83" target="_blank">VGGNet</a></li>
<li><a href="https://homl.info/82" target="_blank">ResNet</a><ul>
<li>Won the 2015 ILSVRC challenge</li>
<li>The CNN is called Residual Network.</li>
</ul>
</li>
<li><a href="https://homl.info/xception" target="_blank">Xception</a><ul>
<li>A variant of GoogLeNet</li>
</ul>
</li>
<li><a href="https://homl.info/senet" target="_blank">SENet</a><ul>
<li>Squeeze-and-Excitation Network</li>
</ul>
</li>
<li>You Only Look Once(YOLO)<ul>
<li><a href="https://homl.info/yolo" target="_blank">YOLO</a></li>
<li><a href="https://homl.info/yolo2" target="_blank">YOLOv2</a></li>
<li><a href="https://homl.info/yolo3" target="_blank">YOLOv3</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="recurrent-neural-networksrnns">Recurrent Neural Networks(RNNs)</h5>
<ul>
<li>RNNs are a type of ANNs that can capture temporal dependencies.<ul>
<li>Temporal Dependencies or Dependencies Over Time - Current output dependent on both current and past inputs.</li>
</ul>
</li>
<li>Most speed recognition, natural language processing(NLP), time series prediction, gesture recognition uses RNNs.<ul>
<li>Gesture recoginition combine CNNs and RNNs.</li>
</ul>
</li>
<li>The inputs for RNNs are in a certain sequence in stead of a single independent input.</li>
<li>The output of a hidden layer will be stored by the previous layer as additional inputs for that hidden layer.<ul>
<li>Thoses additioanl input nodes are the memory that stores the states info of the RNNs.</li>
<li>The value of the previous state node is applied with the activation function.</li>
<li>Each feed back from the previous output is called a time step.</li>
</ul>
</li>
<li>The exploding gradient Problem happens when it grows too large<ul>
<li>It can be solved by gradient clipping which will normalize the gradient when it is over a certain threshold.</li>
</ul>
</li>
<li>RNNs uses Back propagation through time</li>
<li>vanishing gradient problem causes the RNNs only remember inputs from only a few time step.</li>
<li>RNNs has a folded model that has a circular arrow which point to the node itself with memory value <em>s</em>. The model looks the same at any given time.</li>
<li>RNNS has a unfolded model that represent the changes of one node throughout a given time period.<ul>
<li>For unfolded models the input layer is at the bottom the output layer is at the top. Time goes from left to right.</li>
</ul>
</li>
<li>Time Delay Neural Networks(TDNNs) - The first attempt to add memory to the nerual networks.</li>
<li>Simple RNNs or Elman(Jordan) network<ul>
<li>A basic three layer neural network with feedback that serve as memory inputs.</li>
</ul>
</li>
<li>Long Short Term Memory(LSTM)<ul>
<li>solving the problem called vanishing gradient problem of other RNNs in which contributions of information decayed geometrically over time.</li>
<li>It keeps track of both of the long term and long term memeory</li>
<li>It uses four layers to filter and combine long and short term memory. Each layer get input from both current input vector and previous short-term state.<ul>
<li>The main layer is used to generated output that will be added to the current long-term state.</li>
<li>The other three layers are gate controllers for the following gates:<ul>
<li>forget gate - instruct what can be forget from the previous long-term state before adding new info.</li>
<li>input gate - controls what info from the main layer output can be added to the long term state.</li>
<li>output gate - uses a copy of the current long term state to generate the short term state.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Gated Recurrent Networks(GRUs)<ul>
<li>A refined and simplified version of the LSTM.</li>
</ul>
</li>
<li>A Seq2Seq model can be a RNN model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items.<ul>
<li>A typical sequence to sequence model has two parts &#x2013; an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network.</li>
<li>The task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output.</li>
</ul>
</li>
<li>Amazon SageMaker Sequence to Sequence<ul>
<li>A supervised learning algorithm for convert sequence of tokens</li>
<li>the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.</li>
<li>Example applications include: machine translation, text summarization, speech-to-text.</li>
<li>It uses Recurrent Neural Networks (RNNs) and Convolutional Neural Network (CNN) models with attention as encoder-decoder architectures.</li>
</ul>
</li>
</ul>
<h5 id="hyperparameters-tuning">Hyperparameters Tuning</h5>
<ul>
<li>For Neural Networks, hyperparameters tuning can be done during model building and model training.</li>
<li>Build Model<ul>
<li>Hyperparameters in the build model step is called Model Hyperparameters.</li>
<li>Generally, the ideal number of nodes in each hidden layer is close to half of the input layer nodes plus output layer nodes.<ul>
<li>Fine tuning can be made to improve, however this approximate number is good enough for determining the number of nodes for all hidden layers.</li>
<li>The number nodes in each hidden layer determines the capacity of the model to learn a complex model. When the capacity is greater than needed it will learn to much and cause overfitting.</li>
<li>the first hidden layer is suggested to have more nodes than the input layer.</li>
</ul>
</li>
<li>the number of layers<ul>
<li>For the number of hidden layer, 3 layer is way better 2. A 4, 5, 6 layers ANNs won&apos;t necessarily increase the accurancy greatly.</li>
<li>More convolutional layer will increase the accuracy of the prediction.</li>
<li>The more fully connected hidden layers, the better the model predict.</li>
</ul>
</li>
<li>Transfer Learning for CNNs - Use good trained model that can accomplish similar tasks.<ul>
<li>One technique is to remove the last few convolutional layers then train the data since only those layers are reponsible for specific feature detecting for trained model&apos;s own labeling strategy.</li>
<li>Fine-Tuning - replace the last fully connected layers with a new layer then train the model based on existing weight from trained model.</li>
</ul>
</li>
</ul>
</li>
<li>Train Model<ul>
<li>Hyperparameters in the train model step is called Optimizer Hyperparameters.</li>
<li>Epochs<ul>
<li>If the number of epochs is too small the model will be underfitting, If the number of epochs is too large the model will be overfitting.<ul>
<li>The best number of epochs will make the test set and the model have the best performance, this is called the Goldilocks point.</li>
<li>Early Stopping is the algorithm for finding the Goldilocks point, hence determine the best number of epochs to train the model.<ul>
<li>It can be set to stop whenever accurancy decreace, or when accurancy doesn&apos;t improve in the next 10 or 20 epoch.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Learning rate<ul>
<li>can be a value from 0.1 - 0.00001</li>
<li>It controls how fast weight changes during gradient descent.</li>
<li>0.01 suggested starting value.</li>
<li>If to small it will take longer steps to find the best weight, if too big the algorithm might not be able to find the best weight.</li>
<li>Learning rate decay is an algorithm that helps the model to find the best weight.<ul>
<li>it can decrease the learning rate linearly or exponentially after a certain epochs.</li>
<li>Adaptive Learning Rate changes learning rate dynamically based on the data and training result after each epoch.</li>
</ul>
</li>
</ul>
</li>
<li>Minibatch size<ul>
<li>It is 2^n and it can be anywhere from 1 - 256, 32 is a good starting point, then 64, 128, 256.</li>
<li>Larger batch size requires more computational resources, and it might cause out of memory errors.</li>
<li>Smaller batch size helps preventing the weight to be found at local minima, and it will be slow.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
<li>It has an agent</li>
<li>The agent makes observations and takes actions within an environment.<ul>
<li>Observation is a snapshot of the environment states.</li>
</ul>
</li>
<li>An environment can be &quot;solved&quot; when an agent is built to achieve reward above a threshold.</li>
<li>It receives rewards.</li>
<li>The objective of the algorithm is to maximize rewards.<ul>
<li>Rewards can be either positve or negative, only positive, or only negative. Maximize rewards is the same as minimize nagetive reward only, or maximize positive reward only, or both.</li>
<li>The rules to get rewards need to be designed to encourage the agent to make preferred behavior and punished promptly. It is not only about the outcome, the performance need to be quantified.</li>
<li>The punishment should not limit novel strategies which can provides creative solution to gain more rewards.</li>
</ul>
</li>
<li>The algorithm the agent uses to make action is called policy.<ul>
<li>A stochasitc policy is a policy involves randomness.</li>
<li>Policy parameter are the variables that is used to determine the action.</li>
<li>Policy serach is the process of finding the policy parameter that will determine an action that maximize the reward.</li>
<li>Policy space is all the possible combination of the policy parameter.</li>
<li>Policy can be deterministic - all decision is based on a function of state.</li>
<li>Policy can be probabilistic - some decision is made randomly by using probability function(distribution)</li>
</ul>
</li>
<li>After make one action the agent completes one step. After complete all possible actions, the agent completes one episode.<ul>
<li>The set of all possible action in a certain state is called the action space.</li>
</ul>
</li>
<li>Episode is consist of many state, each action will change the state from one to another.<ul>
<li>The set of all possible state is called the state space.</li>
<li>The terminal state is a state that ends an episode.</li>
<li>The duration of an episode is from the initial state to the terminal state.</li>
<li>The state doesnot have to be a single observation, it can be a certain number of obeservation in a group.</li>
</ul>
</li>
<li>When a task is continuous and cannot be measured in an episode, it is called a non-episodic task.(e.g. control room temperature)<ul>
<li>Most tasks can be assumed as episodic tasks.</li>
</ul>
</li>
<li>Generic algorithm is a way to reduce the policy space by generating a certain amount of policy as the first generation and only keep a small portion of the policy which has the most rewards, then generated offsprings from them. Offsprings is a copy of selected policys with some random variation. Keep creating generations until a good policy is found.</li>
<li>Policy gradients(PG) evaluates the relationship between reward and policy parameters, then tweak the parameter following to the gradient towards higher rewards.<ul>
<li>Reinforce Algorithm is one type of PG, It finds the gradients of the probability for each action by leting neural network play the game several times, then compute action&apos;s advantage after several episodes. Multiply the gradient vector by the action&apos;s advantages. Lastly use the mean of all gradient vecotrs to perform gradient descent.</li>
</ul>
</li>
<li>Creating an environment is the first step of RL<ul>
<li>For real world learning, simulated environment is usually used first. The advantages are fast and cheap. Here are the libraries for generating environment.<ul>
<li>Comprehensive environment generating tool: <a href="http://gym.openai.com/" target="_blank">OpenAI Gym</a></li>
<li>3D physics simulation: <a href="https://pybullet.org/" target="_blank">PyBullet</a>, <a href="http://mujoco.org/" target="_blank">MuJoCo</a></li>
</ul>
</li>
</ul>
</li>
<li>A neural network policies uses ANN with an observation(environment states) as input and the probabilities for taking each actions as output.<ul>
<li>If every aspect of the environment is described, then past actions and observations can be ignored.</li>
<li>The action that will be taken based on the output will still be random based on the output&apos;s probabilities, it make the algorithm possible to try all actions with a certain known tendency.</li>
</ul>
</li>
<li>The Credit Assignment Problem - It is used to find out the credit of each step for the final consequence.</li>
<li>The return for an action is a way to evaluate all the consequent rewards followed by an action, the reward immediately after the reward of the current action will have a discount by multiplying the discount factor(&#x1D6FE;), the reward after that second action after the current action will have to multiply the discount factor square and so on, the sum of all the discounted rewards for an action is called the return.<ul>
<li>Discount Factor(&#x1D6FE;) typically ranges from <code>0.9</code> to <code>0.99</code>. The smaller the factor is, the sooner the consequent actions become not important as the factor approaches <code>0</code>.</li>
<li>The normolized returns from a large number of trails(episodes) are called the action advantage. It describes the good or bad of an action compares to all other possible actions.</li>
</ul>
</li>
</ul>
<h4 id="explore-exploit-dilemma">Explore-Exploit Dilemma</h4>
<ul>
<li>When making decision for max reward, two things need to be done.<ul>
<li>Exploration - It the process of gathering data for making decision</li>
<li>Exploitation - Using the known info to make decision to maximize the reward</li>
<li>These two options cannot be done at the same time, this can be called explore-exploit dilemma</li>
</ul>
</li>
<li>Algorithm or methods for solving this dilemma are:<ul>
<li>A/B Testing<ul>
<li>also known as split testing, is the process of comparing two versions of a web page, email, or other marketing asset and measuring the difference in performance.</li>
<li>The performance can be measured using<ul>
<li>click-through rate(CTR), number of click on the page divide by the number view of the page.</li>
<li>conversion rate - the number of sales divided by the total number of visitors.</li>
</ul>
</li>
</ul>
</li>
<li>Epsilon Greedy<ul>
<li>Greedy means picking the bandit with highest Maximum Likehood Estimation(MLE) win rate without considering the sample size and the confidence</li>
<li>Epsilon is the percentage of random chioces this algorithm will make regardless the MLE<ul>
<li><code>&#x3B5;</code> is typically around <code>5%</code> to <code>10%</code></li>
</ul>
</li>
<li>In conclusion Epsilon Greedy Algorithm uses greedy method to make decision with a percentage of random decision</li>
<li><code>&#x3B5;</code> will decrease the reward if it is a constant even when the sample size is big enough to make correct decision. Hence, it can decay over time:<ul>
<li><code>&#x3B5;</code> can be proportional to <code>1/t</code> where <code>t</code> is the number of selection(steps)</li>
<li>Linearlly decay with a minimum value</li>
<li>exponential cooling</li>
<li>one over logarithm</li>
</ul>
</li>
</ul>
</li>
<li>Optimistic Initial Values</li>
<li>UBC1(Upper Confidence Bound)</li>
<li>Thompson Sampling(Bayesian Bandit)</li>
</ul>
</li>
</ul>
<h4 id="markov-decision-processesmdps">Markov Decision Processes(MDPs)</h4>
<ul>
<li>Gridworld is a simple example environment for understanding RL concepts.<ul>
<li>It is a <code>3 X 4</code> table</li>
<li>The state is determined by the location of the agent, the top left corner is described by tuple as <code>(0, 0)</code></li>
<li>An agent starts at the bottom left corner, location <code>(2, 0)</code></li>
<li>The agent move towards all direction available one at a time.</li>
<li><code>(1, 1)</code> is a wall</li>
<li>When the agent get to point <code>(0, 3)</code> it have have one reward.</li>
<li>When the agent get to point <code>(1, 3)</code> it have lost one reward.</li>
</ul>
</li>
</ul>
<h2 id="ready-to-use-algorithms">Ready-to-use Algorithms</h2>
<ul>
<li>fastText<ul>
<li>It is used for text data</li>
<li>fastText is a library for learning of word embeddings and text classification created by Facebook&apos;s AI Research (FAIR) lab.</li>
<li>It is implemented through python ML library <code>Gensim</code></li>
<li>Facebook makes available pretrained models for 294 languages.</li>
<li>fastText uses a neural network for word embedding.</li>
<li>It is extended as BlazingText as an AWS Sagemaker built-in algorithm</li>
<li>It can be a unsupervised learning algorithm using text to vector(Word2Vec)</li>
<li>It can be a supervised learning algorithm that supports multi-class, multi-label classification<ul>
<li>Multi-label can be done with <code>-loss one-vs-all</code> or <code>-loss ova</code>.</li>
</ul>
</li>
<li>Classification is achieved by<ul>
<li>A sentence/document vector is obtained by averaging the word/n-gram embeddings.</li>
<li>For the classification task, multinomial logistic regression is used, where the sentence/document vector corresponds to the features.</li>
<li><a href="https://www.aclweb.org/anthology/E17-2068/" target="_blank">Click</a> to see more.</li>
</ul>
</li>
<li>Gram<ul>
<li>Unigram - each single word is considered as one unit, for a sentence contains words <code>A B C</code>. <code>A</code>, <code>B</code>, <code>C</code> are considered.</li>
<li>Bigram - each two words are considered as one unit, for a sentence contains words <code>A B C</code>. <code>AB</code>, <code>BC</code> are considered.</li>
<li>N-gram - each group of consecutive N words are considered.<ul>
<li>fastText even consider n-gram for group of characters within a word(&quot;ora&quot;, &quot;ran&quot;, &quot;ang&quot;, &quot;nge&quot; when <code>minn</code> is 3) and make it different from <code>Word2Vec</code>. Hence, it works good with misspelled words.</li>
</ul>
</li>
<li>Bigram or N-gram will take the sequence of words into consideration. Unigram will not.</li>
</ul>
</li>
<li>The hierarchical softmax is a loss function that approximates the softmax with a much faster computation.</li>
<li>Word representations - text preprocessing tool<ul>
<li>In order to compute word vectors, a large text corpus is needed.</li>
<li>fastText provides two models for computing word representations:<ul>
<li>skipgram - It predicts the target using a random close-by word of the target word, each close-ly word are considered indiviaully.</li>
<li>cbow (&apos;continuous-bag-of-words&apos;) - It uses the sum of a certain range of close-by word for prediction.</li>
<li>In practice, the skipgram models works better with subword information than cbow.</li>
</ul>
</li>
<li>The subwords are all the substrings contained in a word between the minimum size (minn) and the maximal size (maxn). By default, all the subword are between 3 and 6 characters</li>
<li>The dimension (dim) controls the size of the vectors, the larger they are the more information they can capture but requires more data to be learned.</li>
<li>if they are too large, they are harder and slower to train.</li>
<li>By default, 100 dimensions are used, but any value in the 100-300 range is as popular.</li>
</ul>
</li>
<li>When dealing with languages that does not contain space in between word, word segementation is required.</li>
</ul>
</li>
<li>Object2Vec<ul>
<li>A bulit-in algorithm from the AWS Sagemaker</li>
<li>It utlizes average-pooled embeddings, hierarchical Convolutional Neural Networks (CNNs), as well as multi-layered Bi-Directional-Long-Short-Term-Memory (BiLSTM)-based Recurrent Neural Networks as encoders</li>
<li>It is a supervised learning algorithm for classification, regression.</li>
<li>It is extends Word2Vec</li>
<li>It can capture structure of sentences and learn relationship between pair of objects</li>
</ul>
</li>
<li>Factorization Machines<ul>
<li>A supervised learning algorithm for classification, regression.</li>
<li>Works very well with high dimensional sparse datasets</li>
<li>Popular algorithm for building Recommender systems. Ex, Movie Recommendation based on your viewing habits</li>
<li>It can do collaborative filtering, Ex, cross recommend based on similar users</li>
</ul>
</li>
<li>Amazon Sagemaker Linear Learner<ul>
<li>Supervised ML algorithm.</li>
<li>An AWS sagemaker built-in algorithm for linear regression and logistic regression</li>
<li>It works for for regression, binary classification and multi-class classifcation problems.</li>
</ul>
</li>
<li>XGBoost<ul>
<li>Supervised ML algorithm for regression and classification problems</li>
<li>It is evolved from decision tree in the following order<ol>
<li>Decision Trees - A graphical representation of possible solutions to a decision based on certain conditions.</li>
<li>Bagging - Bootstrap aggregating or Bagging is a ensemble meta-algorithm combining predictions from multiple decision trees through a majority voting mechanism.</li>
<li>Random Forest - Bagging-based algorithm where only a subset of features are selected at random to build a forest or collection of decision trees.</li>
<li>Boosting - Models are built sequentially by minimizing the errors from previous models while increasing(boosting) influence of high-performing models.(New model trained by data points that can&#x2019;t correctly predict by previous model until no further improvement can be made.)</li>
<li>Gradient Boosting - Gradient Boosting employs gradient descent algorithm to minimize errors in sequential models.</li>
<li>XGBoost - Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handing missing values and regularization to avoid overfitting.</li>
</ol>
</li>
<li>Very Popular Algorithm - Won several competitions</li>
</ul>
</li>
<li>AWS Sagemaker DeepAR<ul>
<li>Supervised learning algorithm for timeseries forecasting</li>
<li>The built-in RNN model in AWS Sagemaker</li>
<li>It is the enhanced version of classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series.</li>
<li>Train multiple related time series using a single model</li>
<li>Generate predictions for new, similar timeseries</li>
</ul>
</li>
<li>Amazon Sagemaker Object Detection<ul>
<li>The name of the CNNs model provided by AWS Sagemaker.</li>
<li>A supervised learning algorithm for classification.</li>
<li>It detects and classifies Objects in an image then returns a bounding box of each object location.</li>
</ul>
</li>
<li>Amazon Sagemaker Image Classification<ul>
<li>The name of the CNNs model provided by AWS Sagemaker.</li>
<li>A supervised learning algorithm for classification.</li>
<li>It classify the entire image and it supports multi-labels.</li>
</ul>
</li>
<li>Amazon Sagemaker Semantic Segmentation<ul>
<li>A supervised learning algorithm for classification.</li>
<li>an AWS sagemaker built-in model</li>
<li>Image Analysis Algorithm for Computer Vision Applications</li>
<li>Tags each pixel in an image with a class label</li>
</ul>
</li>
<li>Amazon SageMaker Latent Dirichlet Allocation<ul>
<li>A unsupervised learning algorithm for topic modeling</li>
<li>Group documents by user specified number of topics</li>
<li>For documents, it assigns a probability score for each topic</li>
</ul>
</li>
<li>Amazon SageMaker Neural Topic Model<ul>
<li>Similar to Amazon SageMaker Latent Dirichlet Allocation</li>
</ul>
</li>
</ul>
<h2 id="data-preprocessing">Data Preprocessing</h2>
<h3 id="numeric-data-preprocessing">Numeric Data Preprocessing</h3>
<ol>
<li>Importing the data.</li>
<li>Replacing missing values by the average of all other values.</li>
<li>Encoding categorical data - generating dummy variables.</li>
<li>Splitting the dataset into the Training set and Test set.</li>
<li>Feature Scaling - put all the features on the same scales.<ul>
<li>Not all models need to have feature scaling.</li>
<li>Applied on training set only.</li>
<li>There are two ways for Feature Scaling<ul>
<li>Standardization returns a result from -3 to 3, Standardization alway works.</li>
<li>Normalization returns a result from 0 to 1, Normalization works only if data have a normal distribution in most of the features.</li>
</ul>
</li>
<li>Dummy variables do not need to apply feature scaling.</li>
</ul>
</li>
</ol>
<h3 id="image-preprocessing">Image Preprocessing</h3>
<ul>
<li>The color value of the images can be rescaled from <code>0-255</code> to <code>0-1</code> to simplify the data.<ul>
<li>The training data and testing data should use have the same recaling setting.</li>
</ul>
</li>
<li>Image augumentation step alteres the images by zooming, flipping and shearing to provide variaties for training.</li>
<li>Labeling<ul>
<li><a href="https://github.com/tzutalin/labelImg" target="_blank">LabelImg</a> is a graphical image annotation tool.<ul>
<li>Annotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO format.</li>
</ul>
</li>
<li><a href="https://github.com/microsoft/VoTT" target="_blank">VoTT</a> is an open source annotation and labeling tool for image and video assets.<ul>
<li>VoTT can be installed as a native application or run from source. VoTT is also available as a stand-alone Web application and can be used in any modern Web browser.</li>
<li>they can be exported into a variety of formats:<ul>
<li>Azure Custom Vision Service</li>
<li>Microsoft Cognitive Toolkit (CNTK)</li>
<li>TensorFlow (Pascal VOC and TFRecords)</li>
<li>VoTT (generic JSON schema)</li>
<li>Comma Separated Values (CSV)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="timeseries-data-preprocessing">Timeseries data preprocessing</h3>
<h3 id="text-data-preprocessing">Text data preprocessing</h3>
<ul>
<li>Bag of word - It is a way to preprocess text data by using an array of count for all unique words to record the occurrence of each word in the text data.<ul>
<li>The list of words are ordered by its frenquent among all words in the text data.</li>
<li>Most frenquent words are important, words that are not frenquently seen in the text are usually names, places, holiday etc.</li>
<li>Then, the data can be fed into other algorithms. Ex, ANNs or Native Bayes for classfication.</li>
</ul>
</li>
<li>Word2Vec<ul>
<li>Word2Vec are used to predict a vector representation of a word using a model trained by a text corpus<ul>
<li>text corpus is a language resource consisting of a large and structured set of texts</li>
</ul>
</li>
<li>Word2vec is a group of two-layer neural networks models that are used to produce word embeddings.<ul>
<li>It can be obtained using two methods: Skip Gram or Common Bag Of Words (CBOW)</li>
<li>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.</li>
<li>It uses pre-trained models to convert word to vectors with several hundred dimensions.</li>
</ul>
</li>
<li>Word2Vec is a text preprocessing step for downstream NLP, Sentiment analysis, named entity recognition and translation.</li>
<li>Words that are semantically similar have vectors that are closer to each other</li>
<li>The average of vectorized words in a sentence can the be fed into logistic regression classifiers or any other models.</li>
</ul>
</li>
<li>Tokenization<ul>
<li>Tokenization of raw text is a standard pre-processing step for many NLP tasks. For English, tokenization usually involves punctuation splitting and separation of some affixes like possessives.</li>
<li>Other languages require more extensive token pre-processing, which is usually called segmentation. Some commonly used tool are shown as below: - <a href="https://nlp.stanford.edu/software/segmenter.html" target="_blank">Stanford word segmenter</a> for Chinese and Arabic. - <a href="https://taku910.github.io/mecab/" target="_blank">Mecab</a> for Japanese - <a href="https://github.com/phongnt570/UETsegmenter" target="_blank">UETsegmenter</a> for Vietnamese - <a href="http://www.statmt.org/europarl/" target="_blank">Europarl</a> for Latin, Cyrillic, Hebrew or Greek scripts - ICU tokenizer - <a href="https://arxiv.org/abs/1802.06893" target="_blank">Click</a> to learn more</li>
</ul>
</li>
<li>Text cleanup<ul>
<li>leave out all double qoutes</li>
<li>Stop Words - are words that are not providing useful information and will be excluded from the text.</li>
<li>Stemmer - are used to transform various forms of a word to a common one. Ex, different tense of a verb to the present tense.</li>
<li>Replace all non alphapetic and numerial character to space.</li>
<li>Use lower case for words</li>
</ul>
</li>
<li>GloVe is another unsupervised learning algorithm for obtaining vector representations for words.<ul>
<li>It puts emphasis on the importance of word-word co-occurences to extract meaning rather than other techniques such as skip-gram or bag of words</li>
<li>GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words.</li>
<li>it proves to perform better than Word2vec in the word analogy tasks.</li>
</ul>
</li>
</ul>
<h2 id="model-evaluation">Model Evaluation</h2>
<h3 id="evaluating-regression-models">Evaluating Regression Models</h3>
<h5 id="root-mean-square-errorrmse">Root Mean Square Error(RMSE)</h5>
<ul>
<li>The smaller the better</li>
</ul>
<h5 id="residual-histograms">Residual Histograms</h5>
<ul>
<li>Residual value equals to the actual value minus predicted value.</li>
<li>Residual Histograms have Residual value for each prediction on x-axis and count on y-asix.</li>
<li>It can represent the distribution of the residual if 0 is in the center and the range of x-axis is small, the model is good.</li>
</ul>
<h3 id="evaluating-classification-models">Evaluating Classification Models</h3>
<ul>
<li>Confusion Matrix<ul>
<li>It is a table which has predicted result on one axis and actual result on the other axis, then for a binary classification each prediction has four kinds of outcome:<ul>
<li>True Positive - Correctly predicted a positive result.</li>
<li>True Negative - Correctly predicted a negative result.</li>
<li>False Positive(type I error) - A false prediction which is predicted as Positive.</li>
<li>False Negative(type II error) - A false prediction which is predicted as Negative.</li>
</ul>
</li>
<li>For Multiclass classifier that has <code>n</code> classes, use confusion matrix with shape <code>n X n</code>.</li>
<li>accuracy paradox - It happens when only use accuracy rate as the reference to consider the performance of a model. because the number of the actual positive and negative results does not always be the same.</li>
</ul>
</li>
<li>Area Under Curve (AUC)<ul>
<li>AUC is the area of a curve formed by plotting True Positive Rate against False Positive Rate at different cut-off thresholds</li>
<li>Good model have AUC closer to 1</li>
<li>0.5 is considered random guess</li>
<li>Closer to 0 is unusual and it indicates model is flipping results</li>
</ul>
</li>
<li>CAP<ul>
<li>The number of correct prediction is on the y-axis. The total number of prediction is on the x-axis.</li>
<li>A a random prediction will generate a linear line.</li>
<li>A better model will generate a curve with higher slope at the beginning</li>
<li>There is a perfect linear model that get every prediction correct.</li>
<li>If the area under the line for random model is <code>Sr</code> and <code>Sp</code> for the perfect model, the area under the curve for the model is <code>S</code>. The accuracy rate can be calculated with <code>(S-Sr)/(Sp-Sr)</code>.</li>
<li>A good model can be determined by curve when the x value is in the middle of x-asix the point on the curve will have y-value at above 70% of the y-axis</li>
</ul>
</li>
<li>Use one class again all other classes for multiclass classifier.</li>
</ul>
<h2 id="machine-learning-frameworkslibraries">Machine Learning Frameworks(Libraries)</h2>
<ul>
<li>A ML frameworks is a libray for a certain programming language which contains various ML algorithm as methods.</li>
</ul>
<h4 id="tensorflow"><a href="https://www.tensorflow.org" target="_blank">TensorFlow</a></h4>
<ul>
<li>It was developed by the Google Brain team for internal Google use. It was released under the Apache License 2.0 on November 9, 2015.</li>
<li>It is the most popular deep learning framework for Python</li>
<li>There are also experimental interfaces available in JavaScript, C++, Java and Go, C# and Julia.</li>
<li>TensorFlow can run on multiple CPUs and GPUs</li>
<li>TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.</li>
<li>Static computation graph - The model should be defined first then compile and train it after.</li>
<li>TensorFlow 2.0 uses Keras as its official high-level API</li>
</ul>
<h4 id="pytorch"><a href="https://pytorch.org" target="_blank">PyTorch</a></h4>
<ul>
<li>PyTorch is an open source machine learning library based on the Torch library<ul>
<li>Torch is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language.It provides a wide range of algorithms for deep learning</li>
</ul>
</li>
<li>PyTorch also has a C++ interface</li>
<li>It is developed by Facebook&apos;s AI Research lab (FAIR).</li>
<li>Dynamically Updated Graph - The model is compile after each line of the model definition</li>
<li>PyTorch is suited for small projects and prototyping</li>
</ul>
<h4 id="sonnet"><a href="https://sonnet.readthedocs.io/en/latest/" target="_blank">Sonnet</a></h4>
<ul>
<li>Sonnet is a high level library to build complex neural network structures in TensorFlow.</li>
<li>It is designed to create neural networks with a complex architecture by DeepMind.</li>
</ul>
<h4 id="mxnet"><a href="https://mxnet.apache.org" target="_blank">MXNet</a></h4>
<ul>
<li>It is owned by Apache Software Foundation</li>
<li>The framework initially supports a large number of languages (including C++, Python, Java, Julia, Matlab, JavaScript, Go, R, Scala, Perl, and Wolfram Language.)</li>
<li>MXNet is supported by public cloud providers including Amazon Web Services (AWS) and Microsoft Azure.</li>
<li>Amazon has chosen MXNet as its deep learning framework of choice at AWS.</li>
<li>Apache MXNet is a lean, flexible, and ultra-scalable deep learning framework that supports state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).</li>
<li>MXNet is designed to be distributed on dynamic cloud infrastructure, using a distributed parameter server, and can achieve almost linear scale with multiple GPUs or CPUs.</li>
<li>It is powered by the high-level API library <a href="https://gluon.mxnet.io" target="_blank">Gluon</a>,<ul>
<li>Gluon simplifies the creation of deep learning models.</li>
<li>the Gluon supports work with a dynamic graph.</li>
</ul>
</li>
</ul>
<h2 id="dataset">Dataset</h2>
<ul>
<li>There are many dataset that are ready for various models to use.</li>
<li>Training on the same dataset is a way to compare the quality of a certain mode.</li>
<li>Some competition are hosted based on a certain dataset.</li>
<li>Dataset are often used as the data source for acedemic researches.</li>
<li>See a complete list <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" target="_blank">here</a>.</li>
</ul>
<h3 id="image-data">Image Data</h3>
<h5 id="mnist">MNIST</h5>
<ul>
<li>It contains Database of handwritten digits.</li>
<li>It has 60,000 images, created by National Institute of Standards and Technology in 1998.</li>
</ul>
<h5 id="imagenet"><a href="http://www.image-net.org" target="_blank">ImageNet</a></h5>
<ul>
<li>ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.<ul>
<li>WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.</li>
</ul>
</li>
</ul>
<h5 id="coco"><a href="http://cocodataset.org/#home" target="_blank">COCO</a></h5>
<ul>
<li>Microsoft Common Objects in Context (COCO)</li>
<li>Images are preprocessed with object highlighting, labeling, and classification into 91 object types.</li>
</ul>
<h5 id="ade20k"><a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/index.html" target="_blank">ADE20K</a></h5>
<ul>
<li>Released by MIT CSAIL Computer Vision Group</li>
<li>20210 images are fully annotated with objects and, many of the images have parts too.</li>
<li>2000 images are fully annotated with objects and parts</li>
</ul>
<h3 id="numeric-data">Numeric Data</h3>
<h5 id="iris-dataset"><a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">Iris Dataset</a></h5>
<ul>
<li>It has measurement data for three types of iris plants taken by R. Fisher, released in 1936.</li>
<li>The dataset contains a set of 150 records under 4 attributes - sepal length, sepal width, petal length, petal width.</li>
</ul>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="cryptography.html" class="navigation navigation-prev " aria-label="Previous page: Cryptography">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../math/" class="navigation navigation-next " aria-label="Next page: Mathematics">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Machine Learning","level":"1.20.3","depth":2,"next":{"title":"Mathematics","level":"1.21","depth":1,"path":"math/README.md","ref":"math/README.md","articles":[{"title":"Calculus","level":"1.21.1","depth":2,"path":"math/calculus.md","ref":"math/calculus.md","articles":[]},{"title":"Linear Algebra","level":"1.21.2","depth":2,"path":"math/linear-algebra.md","ref":"math/linear-algebra.md","articles":[]},{"title":"Discrete Mathematics","level":"1.21.3","depth":2,"path":"math/discrete.md","ref":"math/discrete.md","articles":[]},{"title":"Probability & Statistics","level":"1.21.4","depth":2,"path":"math/probability-statistics.md","ref":"math/probability-statistics.md","articles":[]},{"title":"Maple Softwares","level":"1.21.5","depth":2,"path":"math/maple.md","ref":"math/maple.md","articles":[]}]},"previous":{"title":"Cryptography","level":"1.20.2","depth":2,"path":"algorithm/cryptography.md","ref":"algorithm/cryptography.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"algorithm/ml.md","mtime":"2020-08-09T04:34:53.942Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-08-27T21:30:59.692Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

